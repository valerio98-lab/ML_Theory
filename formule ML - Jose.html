<h1 id="machine-learning">Machine learning</h1>
<h2
id="definition-of-a-machine-learning-problem-and-its-goal">Definition of
a machine learning problem and its goal</h2>
<p>The general Machine Learning problem, we have to approximate a
function <span
class="math inline"><em>f</em>‚ÄÑ:‚ÄÑ<em>X</em>‚ÄÑ‚Üí‚ÄÑ<em>Y</em></span>, given a
dataset <span class="math inline"><em>D</em></span> containing
informations about <span class="math inline"><em>f</em></span>. The
specific form of the datasets determines the category of ML problem that
we want to solve.</p>
<h2 id="probabilistic-framework">Probabilistic framework</h2>
<!-- https://dsp.stackexchange.com/questions/53128/why-is-random-noise-assumed-to-be-normally-distributed -->
<p>A key thing that we have to keep in mind in the Machine Learning
setting is the ineherently presence of noise. When we try to model its
probability distribution, using the Gaussian distribution is often a
good choice. Nontheless even if we model it with great accuracy, we
still need to make predictions, thus approaching machine learning from a
probabilistic perspective helps us quantify and manipulate uncerntainty
directly.</p>
<h2 id="supervised-learning">Supervised Learning</h2>
<p>In supervised learning, the dataset <span
class="math inline"><em>D</em></span> comprises, of pairs of input
output</p>
<p><span
class="math display"><em>D</em>‚ÄÑ=‚ÄÑ{‚ÄÑ&lt;‚ÄÑ<em>x</em>,‚ÄÜ<em>y</em>‚ÄÑ&gt;‚ÄÑ|<em>x</em>‚ÄÑ‚àà‚ÄÑ<em>X</em>,‚ÄÜ<em>y</em>‚ÄÑ‚àà‚ÄÑ<em>Y</em>}</span></p>
<h3 id="sample-error">Sample error</h3>
<p>The sample error represents the error that we make on instances
present in the training set.</p>
<p><span class="math display">$$\text{error}_S(h) = \frac{1}{n} \sum_{x
\in S} \delta(f(x) \not ={h(x)})$$</span></p>
<h3 id="true-error">True Error</h3>
<p>We assume that different instances of <span
class="math inline"><em>X</em></span> presented to us in the dataset is
drawn from an unknown distribution <span class="math inline">ùíü</span>.
The true error is the error that the hypotesis make on any value choosen
at random from <span class="math inline">ùíü</span>.</p>
<p><span
class="math display">error<sub><em>D</em></sub>(<em>h</em>)‚ÄÑ=‚ÄÑ<em>P</em><sub><em>x</em>‚ÄÑ‚àà‚ÄÑ<em>D</em></sub>(<em>f</em>(<em>x</em>)‚â†<em>h</em>(<em>x</em>))</span></p>
<p>The true error is impossible to compute, but we can estimate it.</p>
<h3 id="overfitting">Overfitting</h3>
<p>Given a hypotheses space <span class="math inline"><em>H</em></span>,
a hypotheses <span class="math inline"><em>h</em>‚ÄÑ‚àà‚ÄÑ<em>H</em></span>,
is said to overfit the training data if there exists some <span
class="math inline"><em>h</em>‚Ä≤‚ÄÑ‚àà‚ÄÑ<em>H</em></span>, such that:</p>
<p><span
class="math display">error<sub><em>S</em></sub>(<em>h</em>)‚ÄÑ&lt;‚ÄÑerror<sub><em>S</em></sub>(<em>h</em>‚Ä≤)‚ÄÖ‚à©‚ÄÖerror<sub>ùíü</sub>(<em>h</em>)‚ÄÑ&gt;‚ÄÑerror<sub>ùíü</sub>(<em>h</em>‚Ä≤)</span></p>
<p>Overfitting occurs when the hypotesis is too complex compared to the
task it tries to solve.</p>
<h3 id="overfitting-decision-trees">Overfitting Decision Trees</h3>
<p>The depth of a tree controls it‚Äôs complexity. A decision tree that
perfectly classify training examples could lead to overfitting when
there is either noise or/and not enough samples in the dataset. There
are two possible approach this problem</p>
<ol type="1">
<li>Stop growing the tree before it starts to overfit. The decision tree
stops generating nodes when there is no good attribute to split on.</li>
<li>Grow the full tree and then post prune using a statistical
significant test.</li>
</ol>
<p>The second approach is to be preferred, because in the first is
possible that at one given point no particular attribute is the best,
but there are a combination that are informative.</p>
<h3 id="bayesian-learning">Bayesian learning</h3>
<p>The uncertainty is modeled in a bayesian framework. This means that
our beliefs are updated as soon as new data is presented to us. The
bayes theorem is as follows:</p>
<p><span class="math display">$$P(A|B) =
\frac{P(B|A)P(A)}{P(D)}$$</span></p>
<p><span class="math inline"><em>P</em>(<em>A</em>|<em>B</em>)</span> is
the called posterior, beacuse is the updated belief about <span
class="math inline"><em>A</em></span> after taking into consideration
the evidence, <span class="math inline"><em>B</em></span>. <span
class="math inline"><em>P</em>(<em>B</em>|<em>A</em>)</span> is the
likelihood and express how well the observed data supports our
hypotesis. <span class="math inline"><em>P</em>(<em>A</em>)</span> is
the prior, reflects the prior beliefs we had before evidence was
presented to us.</p>
<h3 id="maximum-a-posteriori-hypothesis">Maximum a Posteriori
Hypothesis</h3>
<p>Is usual that a particular learning algorithm returns not a single
hypotesis but a set of hypotesis. Our objective would be to determine
the most probable hypotesis <span class="math inline"><em>h</em></span>
given the data at hand <span class="math inline"><em>D</em></span>. In
other word we would like to determine hypotesis that maximizes the
posterior.</p>
<p><span class="math display">$$h_{MAP} = \underset{h \in
H}{\text{argmax}} P(h | D) \\$$</span> <span
class="math display">$$\stackrel{(1)}{=} \underset{h \in
H}{\text{argmax}} \frac{P(D|h)P(h)}{P(D)} \\$$</span> <span
class="math display">$$\stackrel{(2)}{=} \underset{h \in
H}{\text{argmax}}  {P(D|h)P(h)}$$</span></p>
<ol type="1">
<li>Is given by Bayes Theorem.</li>
<li>Is given by the fact that <span
class="math inline"><em>P</em>(<em>D</em>)</span> is a constant and
<span class="math inline">argmax</span> is invariant to constant
multiplication.</li>
</ol>
<h3 id="maximum-likelihood">Maximum Likelihood</h3>
<p>Assuming we know the prior probability of each hypotesis <span
class="math inline"><em>h</em></span>, we can determine the most
probable hypotesis by computing the <span
class="math inline"><em>h</em><sub><em>M</em><em>A</em><em>P</em></sub></span>.
However the knowledge of <span
class="math inline"><em>P</em>(<em>h</em>)</span>, might not be
available, thus we have no reason to think that a particular hypotesis
must be preferred over another, so we model it as a uniform
distribution, thus <span
class="math inline"><em>P</em>(<em>h</em>)</span> can be ignored,
because it becomes a constant value.</p>
<p><span class="math display">$$h_{ML} =\underset{h \in
H}{\text{argmax}} P(h \mid D) = \argmax P(D\mid h)$$</span></p>
<h3 id="optimal-bayes-classifier">Optimal Bayes Classifier</h3>
<p>While the <span
class="math inline"><em>h</em><sub><em>M</em><em>A</em><em>P</em></sub></span>
is the most probable hypotesis, given the data, it‚Äôs classification
might not be the most probable. The Bayes Optimal Classifier on the
other hand classify each instance with its most probable value. The BOC,
determines the most probable value by making a weighted sum of the
probability of a specific value <span
class="math inline"><em>v</em></span>, assuming that <span
class="math inline"><em>h</em></span> is true, weighted by the
probability of <span class="math inline"><em>h</em></span> being true
given the data <span class="math inline"><em>D</em></span>.</p>
<p><span class="math display">$$v_{obc} = \underset{h \in
H}{\text{argmax}} \sum_{hi \in H} P(v | h)P(h | D)$$</span></p>
<p>The BOC, takes it‚Äôs name from the fact that under the same hypotesis
space, and with the same a priori knowledge no other method outperforms
it on average. It is, however, not practical in real situations, due to
its computationally intensive nature.</p>
<h3 id="naive-bayes-classifier">Naive Bayes classifier</h3>
<p>The Naive Bayes Classifier is a practical algorithm. It can be used
under the assumption that any value <span
class="math inline"><em>v</em>‚ÄÑ‚àà‚ÄÑ<em>V</em></span>, where <span
class="math inline"><em>V</em></span> is a finite set, that we want to
compute, can be expressed as a conjunction of its attributes. Since
every value can be described as a conjunction of its attributes, the
naive Bayes algorithm determines <span
class="math inline"><em>v</em><sub><em>m</em><em>a</em><em>p</em></sub></span>
in the following way:</p>
<p><span class="math display">$$v_{MAP} = \underset{h \in
H}{\text{argmax}}  P(v | a_1, a_2, .... , a_n) \\$$</span></p>
<p><span class="math display">$$\stackrel{(1)}{=} \underset{h \in
H}{\text{argmax}} P( a_1, a_2, .... , a_n | v) \\$$</span> <span
class="math display">$$\stackrel{(2)}{=} \underset{h \in
H}{\text{argmax}} P(v) \prod_i P(a_i | v) \\$$</span></p>
<ol type="1">
<li>Comes from the combined application of Bayes Theorem and that <span
class="math inline"><em>P</em>(<em>a</em><sub>1</sub>,‚Ä¶,<em>a</em><sub><em>n</em></sub>)</span>
is a constant.</li>
<li>Comes from the Independence assumption. The independence assumptions
states that attributes values are conditionally independent given the
target value.</li>
</ol>
<h3 id="naive-bayes-as-approximation-of-the-obc">Naive Bayes as
approximation of the OBC</h3>
<p>Naive Bayes is considered an approximation of the Bayes optimal
classifier because it simplifies the joint probability calculation by
assuming conditional independence between features given the class.
While this assumption may not hold in every case, Naive Bayes remains a
practical and effective classification algorithm, providing a
computationally efficient way to approach the optimal Bayes classifier
in situations where the independence assumption is reasonable.</p>
<h3 id="document-classification">Document classification</h3>
<p><span
class="math display">doc<sub><em>i</em></sub>‚ÄÑ=‚ÄÑ{<em>a</em><em>b</em><em>s</em><em>t</em><em>r</em><em>a</em><em>c</em><em>t</em>‚ÄÖ‚à™‚ÄÖ<em>t</em><em>i</em><em>t</em><em>l</em><em>e</em>‚ÄÖ‚à™‚ÄÖ<em>a</em><em>u</em><em>t</em><em>h</em><em>o</em><em>r</em>‚ÄÖ‚à™‚ÄÖ<em>p</em><em>u</em><em>b</em><em>b</em><em>l</em><em>i</em><em>c</em><em>a</em><em>t</em><em>i</em><em>o</em><em>n</em>}</span></p>
<p>We can now set up a dataset vocabulary.</p>
<p>So given a new doc doc_i we want to compute:</p>
<p><span class="math display">$$\text{V}_{NB} = \argmax P(cj \mid D)
\prod_{i=1}^{n} P(ai \mid cj, D)$$</span> <span class="math display">$$=
\argmax P(cj \mid D) P(d \mid cj, D)$$</span></p>
<p>We use an approaching bag of words (BoW) based on a multinomial
distribution for multiclass problem.</p>
<p>We represent a doc as a fixed-lenght feature vector d given that we
have</p>
<p><span
class="math display"><em>d</em>‚ÄÑ=‚ÄÑ‚ÄÑ&lt;‚ÄÑ<em>d</em>1,‚ÄÜ...,‚ÄÜ<em>d</em><em>n</em>&gt;</span></p>
<p>where d_i = k if word i occurs k time in doc_i</p>
<p>So for each feature we compute:</p>
<p><span class="math display">$$P(d|c_j, D) = \frac{n!}{d_1! \dots d_n!}
\prod_{i=1}^{n} P(w_i|c_j, D)^{d_i} $$</span></p>
<p>Maximum-likelihood solution:</p>
<p><span class="math display">$$\hat{P}(w_i|c_j, D) =
\frac{\sum_{\text{doc} \in D} tf_{i,j} + \alpha}{\sum_{\text{doc} \in D}
tf_j + \alpha \cdot |V|} $$</span></p>
<p>dove:</p>
<ol type="1">
<li><span class="math display">$$\text{tf}_{i,j} : \text{numero di
occorrenze di \( w_i \) nel documento doc della classe \( c_j
\)}.$$</span></li>
<li><span class="math display">$$\text{tf}_{j} : \text{frequenza di
tutti i termini del documento \( \text{doc} \) della classe \( c_j
\)}.$$</span></li>
<li><span class="math display"><em>Œ±</em>: parametro di
smoothing.</span></li>
</ol>
<h3 id="linear-regression">Linear Regression</h3>
<p>Given:</p>
<p><span class="math display">$$y(x,\bold{w}) = w_0 + w_1x_1 + ... +
w_dx_d = \bold{w}^Tx$$</span></p>
<p>Due to noise, we can say that:</p>
<p><span class="math display">$$t = y(x,\bold{w}) +
\epsilon$$</span></p>
<p>The optimal value of <span class="math inline">$\bold{w}$</span> can
be determined using a maximum likelihood approach. Assuming <span
class="math inline"><em>œµ</em></span> is Gaussian error, and that
observations are i.i.d we can define the likelihood function of <span
class="math inline">$\bold{t}$</span> as:</p>
<p><span class="math display">$$P( \bold{t} \mid \bold{x}, \bold{w},
\beta) = \prod_{i=1}^{n} N(t_i\mid y(x_i,\bold{w}),
\beta^{-1})$$</span></p>
<p>Insted of maximizing this quantity we can maximize the log
likelihood:</p>
<p><span class="math display">$$\underset{\bold{w}}{\text{argmax}}(
\log( P( \bold{t} \mid \bold{x}, w, \beta)))  \\$$</span> <span
class="math display">$$= \underset{\bold{w}}{\text{argmax}} (\frac{N}{2}
\log {\beta} - \frac{N}{2}\log (2\pi) - \beta
E_D(\bold{w}))$$</span></p>
<p><span class="math display">$$\stackrel{(1)}{=}
\underset{\bold{w}}{\text{argmax}} (E_D(\bold{w}))$$</span> <span
class="math display">$$\stackrel{(2)}{=}
\underset{\bold{w}}{\text{argmin}} (E_D(\bold{w}))$$</span></p>
<ol type="1">
<li>Comes from the fact that argmax is invariant to constant addition
and multiplication.</li>
<li>Come from the fact that maximization of the log likelihood is
equivalent to the minimization of negative log likelihood. Where</li>
</ol>
<p><span class="math display">$$E_d(\bold{w}) = \frac{1}{2}
\sum_{i=1}^{N} ({t_i - \bold{w}^Tx_i})^2$$</span></p>
<p>In order to find the minimum we need to differentiate:</p>
<p><span class="math display">$$\nabla E_d(\bold{w}) = \sum_{i=1}^{N}
(t_i - \bold{w}^T x_i)x_i^T$$</span></p>
<p>setting to zero:</p>
<p><span class="math display">$$0 = \sum_{i=1}^{N} t_ix_i^T - \bold{w}^T
(\sum_{i=1}^{N}{x_{i}x_{i}^T})$$</span></p>
<p>Gives us the maximum likely solution, which can be written in closed
form.</p>
<p><span class="math display">$$\bold{w}_{ML} =
(\bold{X}^T\bold{X})^{-1} \bold{X}^T \bold{t} $$</span></p>
<p>Determining the optimal values using the closed form can be costly,
due to the fact that, it needs to process the entire dataset. We can
define an iterative approach, by updating the weights based on a subset
(mini batch) or only a single element (sequential) through the following
learning rule.</p>
<p><span class="math display">$$\bold{w} ^{\tau} \leftarrow
\bold{w}^{\tau - 1} - \eta \nabla{E_n}$$</span></p>
<p>Where <span
class="math inline"><em>E</em><sub><em>n</em></sub></span> is the error
averaged over the subset.</p>
<h3 id="logistic-regression">Logistic Regression</h3>
<p>Utilizing as a functional form, the GLM we can model the class
conditional probbailities and our model as follow:</p>
<p><span class="math display">$$P(C_1 \mid \bold{x}) = y(\bold{x}) =
\sigma(\bold{w}^T\bold{x})$$</span> <span class="math display">$$P(C_2
\mid \bold{x}) = 1 - P(C_1 \mid \bold{x})$$</span></p>
<p>we can express the likelihood as</p>
<p><span class="math display">$$P(\bold{t} \mid \bold{w}) =
\prod_{n=1}^{N}
y(\bold{x}_n)^{t_n}(1-y(\bold{x}_n))^{1-t_n}$$</span></p>
<p>Since the logarithm is a non monotonic function, we can use the
maximum likelihood approach and determine the optimal values by choosing
<span class="math inline">$\bold{w}^*$</span> as the <span
class="math inline">$\bold{w}$</span> that maximizes the log likelihood.
Also since maximizing a function is the same thing as minimizing
negative:</p>
<p><span class="math display">$$- \underset{\bold{w}}{\text{argmin}}
(\ln P(\bold{t} \mid \bold{w})) = -\sum_{n = 1}^{N} [t_n \ln
y(\bold{x}_n) + (1-t_n)\ln (1-y(\bold{x}_n))] = E(\bold{w})$$</span></p>
<p>Which is the negative cross entropy.</p>
<h3 id="gram-matrix">Gram Matrix</h3>
<p>A Gram Matrix is an <span
class="math inline"><em>N</em>‚ÄÖ√ó‚ÄÖ<em>N</em></span> symmetric matrix with
elements:</p>
<p><span
class="math display"><em>K</em><em>n</em><em>m</em>‚ÄÑ=‚ÄÑ<em>x</em><sub><em>n</em></sub><sup><em>T</em></sup><em>x</em><sub><em>m</em></sub>‚ÄÑ=‚ÄÑ<em>K</em>(<em>x</em><em>n</em>,<em>x</em><em>m</em>)</span>
<span class="math display">$$
K = \begin{bmatrix}
    K(x_1, x_1) &amp; K(x_1, x_2) &amp; \cdots &amp; K(x_1, x_n) \\
    K(x_2, x_1) &amp; K(x_2, x_2) &amp; \cdots &amp; K(x_2, x_n) \\
    \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
    K(x_n, x_1) &amp; K(x_n, x_2) &amp; \cdots &amp; K(x_n, x_n)
\end{bmatrix}
$$</span> The entries of the Gram matrix correspond to the pairwise
similarities between data points, and this matrix is employed to compute
decision boundaries in the transformed feature space.</p>
<h3 id="definition-of-kernel">Definition of kernel</h3>
<p><span class="math inline"><em>k</em></span> is a kernel function
defined as a real-valued function, simmetric and non negative. <span
class="math inline"><em>k</em>(<em>x</em>,<em>x</em>‚Ä≤)</span> with <span
class="math inline"><em>x</em>,‚ÄÜ<em>x</em>‚Ä≤‚ÄÑ‚àà‚ÄÑ<em>X</em></span> defines
similarity measure between instances <span
class="math inline"><em>x</em></span> and <span
class="math inline"><em>x</em>‚Ä≤</span>.</p>
<h3 id="kernel-trick">Kernel Trick</h3>
<p>If an input vector <span class="math inline">$\bold{x}$</span>,
appers in an algorithm only as an inner product <span
class="math inline"><em>x</em><sup><em>T</em></sup><em>x</em>‚Ä≤</span>,
then the inner product can be replaced with some kernel function <span
class="math inline"><em>k</em>(<em>x</em>,<em>x</em>‚Ä≤)</span></p>
<h3
id="determination-of-kernelized-version-for-linear-regression-without-regularization">Determination
of kernelized version for linear regression without regularization</h3>
<p>Given a linear model</p>
<p><span class="math display">$$y(x,\bold{w}) = w_0 + w_1x_1 + ... +
w_dx_d = \bold{w}^Tx$$</span></p>
<p>We know, by using the maximum likely approach that a suitable error
function, is the least square error</p>
<p><span class="math display">$$E_d(\bold{w}) = \frac{1}{2}
\sum_{i=1}^{N} ({t_i - \bold{w}^Tx_i})^2$$</span></p>
<p>By setting it to <span class="math inline">0</span> we get the closed
form of <span class="math inline">$\bold{w}^*$</span>:</p>
<p><span class="math display">$$\bold{w}^* =
(\bold{X}{\bold{X}^T})^{-1}\bold{X}^T\bold{t} = \alpha \bold{X}^T =
\sum_{n=1}^{N} \alpha_nx_n $$</span></p>
<p>Were <span class="math inline"><em>Œ±</em></span> is defined as <span
class="math inline">$\bold{K}^{-1}\bold{t}$</span> and <span
class="math inline">$\bold{K}$</span> is the Gram matrix. Putting <span
class="math inline">$\bold{w}^*$</span> back into the model we
obtain:</p>
<p><span class="math display">$$y(x,\bold{w}^*) = \bold{w}^{*^T}x =
(\alpha\bold{X})^{T}  x  = \sum_{n=1}^N \alpha_nx_n^Tx $$</span></p>
<p>By using the kernel trick:</p>
<p><span class="math display">$$y(x,\bold{w}^*) =  \sum_{n=1}^N
\alpha_nk(x_n,x) $$</span></p>
<h3
id="determination-of-kernelized-version-for-linear-regression-with-regularization">Determination
of kernelized version for linear regression with regularization</h3>
<p>Given a linear model</p>
<p><span class="math display">$$y(x,\bold{w}) = w_0 + w_1x_1 + ... +
w_dx_d = \bold{w}^Tx$$</span></p>
<p>We know, by using the maximum likely approach that a suitable error
function, is the least square error</p>
<p><span class="math display">$$E_d(\bold{w}) = \frac{1}{2}
\sum_{i=1}^{N} ({t_i - \bold{w}^Tx_i})^2 + \lambda
||\bold{w}||^2$$</span></p>
<p>Where <span class="math inline"><em>Œª</em></span> is a regularization
term that controls the model complexity. By setting it to <span
class="math inline">0</span> we get the closed form of <span
class="math inline">$\bold{w}^*$</span>:</p>
<p><span class="math display">$$\bold{w}^* = (\bold{X}{\bold{X}^T}
+\lambda \bold{I_n})^{-1}\bold{X}^T\bold{t} = \alpha \bold{X}^T =
\sum_{n=1}^{N} \alpha_nx_n $$</span></p>
<p>Were <span class="math inline">$\bold{\alpha}$</span> is defined as
<span class="math inline">$(\bold{K}+\lambda
\bold{I_n})^{-1}\bold{t}$</span> and <span
class="math inline">$\bold{K}$</span> is the Gram matrix. Putting <span
class="math inline">$\bold{w}^*$</span> back into the model we
obtain:</p>
<p><span class="math display">$$y(x,\bold{w}^*) = \bold{w}^{*^T}x =
(\alpha\bold{X})^{T}  x  = \sum_{n=1}^N \alpha_nx_n^Tx $$</span></p>
<p>By using the kernel trick:</p>
<p><span class="math display">$$y(x,\bold{w}^*) =  \sum_{n=1}^N
\alpha_nk(x_n,x) $$</span></p>
<h3 id="generative-vs-discriminant-model">Generative vs Discriminant
Model</h3>
<p>In a probabilistic framework the joint probability <span
class="math inline">$P(\bold{x}\bold{t})$</span> gives us the most
complete information. However determining it directly is often an
unfeasible task. We can then use two different approaches. A generative
approach aims to understand how data is generated, it does so by
learning the class conditional densities of each class <span
class="math inline"><em>P</em>(<em>x</em>‚à£<em>C</em><sub><em>k</em></sub>)</span>,
the prior <span
class="math inline"><em>P</em>(<em>C</em><sub><em>k</em></sub>)</span>,
and then determining the posterior <span class="math inline">$P(C_k \mid
\bold{x})$</span>.</p>
<p>A discriminative approach are interessed in determining the decision
boundary. This can be done in a probabilistic way, for example linear
regression uses the form of the class conditional probability <span
class="math inline"><em>P</em>(<em>C</em><sub><em>k</em></sub>‚à£<em>x</em>)</span>
of the Generalized linear models and find the best parameters by using a
maximum likelihood approach. Another approach is to directly determine
de decision boundary without manipulating explicitly probabilities.</p>
<h3 id="perceptron">Perceptron</h3>
<p>The perceptron is a simple binary classification algorithm, designed
to mimic the way a biological neuron works. The model coreresponds to a
linear combination of the inputs, that are then passed to a function
<span class="math inline"><em>o</em></span> <span
class="math display">$$y(\bold{x},\bold{w}) = o(\bold{w}^T\bold{x})
$$</span></p>
<p>The <span class="math inline"><em>o</em></span> function is just the
sign function</p>
<p><span class="math display">$$
o(\mathbf{x}) =\text{sign}(\mathbf{w}^T\mathbf{x})=
\begin{cases}
1 &amp; \text{if } \mathbf{w}^T\mathbf{x} &gt; 0 \\
-1 &amp; \text{otherwise}
\end{cases}
$$</span></p>
<p>This function however is piecewise and thus not differentiable, this
means that in order to find the optimal values of <span
class="math inline">$\bold{w}$</span> we need to define an alternative
method, called perceptron critetion. The output of the sign function is
either <span class="math inline">‚ÄÖ‚àí‚ÄÖ1</span> or <span
class="math inline">1</span>, thus a condition that must be satisfied is
<span class="math inline">$\bold{w}^T\bold{x}_i\bold{t}_i &gt;
0$</span>. The perceptron criterion aissngs <span
class="math inline">0</span> to any pattern correctly classifiedand tris
to minimize the following error function:</p>
<p><span class="math display">$$E(\bold{w})= - \sum_{n \in \mathcal{M}}
\bold{w}\bold{x}_n \bold{t}_n$$</span></p>
<p>where <span class="math inline">‚Ñ≥</span> is the set of misclassified
examples. It optimizes <span class="math inline">$E(\bold{w})$</span>
using SGD. According to the perceptron convergence theorem we know that
if the data is linearly separable and the learning rate <span
class="math inline"><em>Œ∑</em></span> is chosen corretly the algorithm
is guaranteed to finda solution in a finite amount of steps, this
however does not guarantees that the time is acceptable. The initial set
of weights is choosen at random, meaning that at the beginning we are
most certainly in a bad situation, however if we choose a learning rate
small enough we are guaranteed to improve our situation. The problem is
that in a linearly separable dataset, there may exist an infinite number
of solutions, and since the movement is slow, due to the learning rate
is probable that we get a decision boundary near the data points,
leading to a decision boundary with poor generalization
performances.</p>
<h3 id="k-nn">K-NN</h3>
<p>KNN is a non parametric model that uses an instance based approach.
In KNN the instances of the dataset are the parameters, thus KNN has a
variable number of parameters, this mean that there is no training
phase, but also that it requires a lot of memory because to make a
prediction we need the entire subset. Given a new instance <span
class="math inline">$\bold{x}$</span>, a value <span
class="math inline"><em>k</em></span> and a distance metric the
classification value is chosen as follow:</p>
<ul>
<li>Find K-Nearest neighbors of <span
class="math inline">$\bold{x}$</span>, according to the distance
metric</li>
<li>Assign to <span class="math inline"><em>x</em></span> the most
common class among the k-nearest neighbors</li>
</ul>
<h3 id="svm">SVM</h3>
<p>The Support Vector Machines makes classification using the maximum
margin. The margin is the distance between the decision boundary and its
closest point. The maximum margin approach can be motivated as follow.
Even if the dataset is linearly separable, there may exist many
solutions, since our goal is not to determine a decision boundary but
rather to determine the decision boundary that generalizes better. Since
the magin is maximum we can achieve a lowe generalization error, beaause
we have ‚Äúmore room‚Äù for error. The fact that the maximum margin yields a
lowe generalization error also means that SVM are less prone to
overfitting.</p>
<h3 id="kernelized-svm">Kernelized SVM</h3>
<p>Given the maximum margin hyper plane, classification new instance
<span class="math inline">$\bold{x}'$</span> is performed using by</p>
<p><span class="math display">$$y(\bold{x}') = \text{sign}( w_0^* +
\sum_{k, x_k\in SV} a_k^*t_{k} \bold{x}'^T \bold{x}_k )$$</span></p>
<p>But since the input vector, is present only as inner product we can
the condition to use the kernel trick is met and we can rewrite as</p>
<p><span class="math display">$$y(\bold{x}') = \text{sign}( w_0^* +
\sum_{k, x_k\in SV} a_k^*t_{k} k(\bold{x}'^T, \bold{x}_k) )$$</span></p>
<h3 id="soft-margins">Soft margins</h3>
<p>If the dataset is not perfectly linearly separable, class conditional
distributions may overlap and SVM will result in poor generalization. We
can relax the SVM, allowing to make some classifications. It is
intuitive to understand that, misclassified points are points that are
on the wrong side of the decision boundary. Another intuitive thing is
that the farther from the decision boundary the point is, the worst its
classification. We formalize this concept through the we introduction of
slack variables <span class="math inline"><em>Œæ</em></span>. Slack
variables mark a penalty that increases as the distance from the
decision boundary grows.</p>
<h3 id="svm-for-regression">SVM for regression</h3>
<p>Whe can use SVM for regression. In machine learning, and more
specifically in supervised regression problems, we know we have to deal
with noise. Thus if we make a model we know that is possible that the
error we make is non zero. We express this concept utilizing the
following error function:</p>
<p><span class="math display">$$J(w, C) = C \sum_{i=1}^{N}
L_{\epsilon}(t_i, y_i) + \frac{1}{2} \| w \|^2$$</span></p>
<p>In which the term <span
class="math inline"><em>L</em><sub><em>œµ</em></sub></span> is called
<span class="math inline"><em>œµ</em></span>-insensitive error
function:</p>
<p><span class="math display">$$
L_{\epsilon}(t, y) =
\begin{cases}
0 &amp; \text{se } |t-y|&lt;\epsilon \\
|t-y|-\epsilon &amp; \text{else}
\end{cases}
$$</span></p>
<p>The result of using the <span
class="math inline"><em>œµ</em></span>-insensitive error is that there is
an immaginary tube, called <span
class="math inline"><em>œµ</em></span>-tube in which the predictions are
assumed correct. The problem is that <span
class="math inline"><em>œµ</em></span>-insensitive error function is
piecewise linear and thus not differentiable. We can solve this problem
by introducing two slack variables <span
class="math inline"><em>Œæ</em></span> and <span
class="math inline"><em>ŒæÃÇ</em></span> which represents how much we
sthrive from the <span class="math inline"><em>œµ</em></span>-tube.
Constraining <span class="math inline"><em>Œæ</em>‚ÄÑ‚â•‚ÄÑ0</span> and <span
class="math inline"><em>ŒæÃÇ</em>‚ÄÑ‚â•‚ÄÑ0</span> we can reexpress the oerror
function as</p>
<p><span class="math display">$$J(w, C) = C \sum_{i=1}^{N} (\xi_n+
\hat{\xi}_n) + \frac{1}{2} \| w \|^2$$</span></p>
<p>Which can be minimized</p>
<h2 id="unsupervised-learning">Unsupervised learning</h2>
<h3 id="expectation-maximization">Expectation Maximization</h3>
<p>The EM algorithm is used to find maximum likelihood parameters of a
statistical model in cases where the equations cannot be solved
directly. Typically these models involve latent variables in addition to
unknown parameters and known data observations. That is, either missing
values exist among the data, or the model can be formulated more simply
by assuming the existence of further unobserved data points. For
example, a mixture model can be described more simply by assuming that
each observed data point has a corresponding unobserved data point, or
latent variable, specifying the mixture component to which each data
point belongs. In its most basic form the EM algorithm works as
follows:</p>
<ul>
<li>Initialize the parameters that need to be maximized</li>
<li>Until termination: E. Estimate the missing variables in the dataset
M. Maximize the parameters of the model in the presence of the data</li>
</ul>
<h3 id="gaussian-mixture-model">Gaussian Mixture Model</h3>
<p>The Gaussian distribution is a versatile distribution, however being
unimodal (has one maximum) it may not be a good fit for specific
problems. We can extend the Gaussian distribution with the concept of
mixture distribution or more specifically Gaussian Mixture Models. We
can define a GMM as a linear superposition of <span
class="math inline"><em>K</em></span> Gaussian</p>
<p><span class="math display">$$P(\bold{x}) = \sum_{k=1}^{K}
\pi_{k}\mathcal{N}(x \mid \mu_{k}, \Sigma_{k})$$</span></p>
<p>Where <span
class="math inline"><em>œÄ</em><sub><em>k</em></sub></span> is called
mixing coeffient and can be thought of the prior probability of picking
the point from the <span class="math inline"><em>k</em></span>-th
Gaussian. And <span
class="math inline"><em>Œº</em><sub><em>k</em></sub></span> and <span
class="math inline"><em>Œ£</em><sub><em>k</em></sub></span> are the mean
and covariance for the <span class="math inline"><em>k</em></span>-th
Gaussian.</p>
<p>The form of the GM distribution is governed by the parameters, one
way to set them is through the maximum likelihood approach, however due
to the summation of gaussians is not possible to write a solution in
closed form and the solution is determined through the EM algorithm.</p>
<p>When visualizing this parameters it might be useful to remember that
the variance of a gaussian <span class="math inline"><em>Œ£</em></span>
can be expressed in its <span class="math inline"><em>D</em></span>
components as follow:</p>
<p><span class="math display">$$\Sigma = \sum_i=1^D
\lambda_i\bold{u}_i\bold{u}_i^T$$</span></p>
<p>Where <span
class="math inline"><em>Œª</em><sub><em>i</em></sub></span> are the eigen
values of <span class="math inline"><em>Œ£</em></span> <img
src="./media/cov_visualization.png" alt="Alt text" /></p>
<h3 id="mathematical-formulation-of-k-means">Mathematical formulation of
K-Means</h3>
<p>Assuming we have a dataset comprised of <span
class="math inline"><em>N</em></span> instances, our goal is to
determine <span class="math inline"><em>K</em></span> clusters. We can
intuitively think of clusters of group of data points whose inter-point
distance are smalled comparedwith the distance to points outside the
cluster. To formalize this concept we introduce three things</p>
<ul>
<li>A vector <span class="math inline"><strong>Œº</strong></span> where
<span
class="math inline"><strong>Œº</strong><sub><em>k</em></sub></span>,
which will represent the mean of the the <span
class="math inline"><em>k</em></span>-th cluster</li>
<li>A vector of <span class="math inline"><em>n</em></span> <span
class="math inline"><em>k</em></span>-dimensional vectors <span
class="math inline">$\bold{r}$</span>, in which the <span
class="math inline"><em>k</em></span>-th element is <span
class="math inline">1</span> and the other <span
class="math inline"><em>k</em>‚ÄÖ‚àí‚ÄÖ1</span> are <span
class="math inline">0</span></li>
<li>An objective function, called distrotion measure $J = <em>{n=1}^N
</em>{k=1}^K r_{nk}|| - ||^2 $</li>
</ul>
<p>After <span class="math inline"><em>K</em></span> is decided the
pseudo works as follow:</p>
<ol type="1">
<li>Set <span class="math inline"><strong>Œº</strong></span> and <span
class="math inline">$\bold{r}$</span> at random</li>
<li>While not termination condition is met: <strong>E.</strong> Optimize
<span class="math inline"><em>J</em></span> w.r.t to <span
class="math inline">$\bold{r}$</span> keeping <span
class="math inline"><strong>Œº</strong></span> fixed. In other words
assign the <span class="math inline"><em>n</em></span>-th data point to
the closer cluster. <strong>M.</strong> Optimize <span
class="math inline"><em>J</em></span> w.r.t to <span
class="math inline"><strong>Œº</strong></span> keeping <span
class="math inline">$\bold{r}$</span> fixed. By optimizing w.r.t to
<span class="math inline"><strong>Œº</strong></span> we implicitly
determine the mean for each cluster.</li>
</ol>
<h3 id="intrinsic-dimension">Intrinsic dimension</h3>
<p>The intrinsic dimension for a data set can be thought of as the
number of variables needed in a minimal representation of the data.</p>
<h3 id="pca">PCA</h3>
<p>Is an tequique widely used for applications such as</p>
<ol type="1">
<li>Dimensionality reduction</li>
<li>Data compression</li>
<li>Data visualization</li>
<li>Feature extraction</li>
</ol>
<p>There are two definitions that lead to the same algorithm. Given a
<span class="math inline"><em>D</em></span> dimensional feature space,
dimension <span class="math inline"><em>M</em></span> such that <span
class="math inline"><em>M</em>‚ÄÑ&lt;‚ÄÑ<em>D</em></span> and an instance
<span class="math inline">$\bold{x}_D$</span> we can express <span
class="math inline">$\bold{x}_D$</span> with <span
class="math inline"><em>M</em></span> dimensions taking the <span
class="math inline"><em>M</em></span> principal components.</p>
<p><span class="math display">$${\bold{x}_M} = \sum_{i=1}^{M}
(\bold{x}_D^T u_{i})u_{i}$$</span></p>
<p>The intrinsic dimensions of a dataset impose a lower bound under
which we canno reconstruct the input (apart from noise) anymore.</p>
<h4 id="maximum-variance-formulation">Maximum Variance Formulation</h4>
<p>PCA can be defined as the ortogonal projection of data into a lower
dimensional linear space, called principal subspace, such that the
variance of the projected data is maximized.</p>
<p>The idea behind Maximum Variance PCA is that directions in which
variance is maximized are more informative. <img
src="./media/pca_MV.png" alt="Alt text" /></p>
<p>We need to determine <span class="math inline">$\overline{x}$</span>
the mean of our dataset and subtract it so the dataset has now <span
class="math inline">0</span> mean. We determine the covariance of the
mean <span class="math inline">0</span> dataset. Covariance can be
written as an expansion in terms of its eigenvalues.</p>
<p><span class="math display">$$\boldsymbol{\Sigma} = \sum_{i=1}^D
\lambda_i\bold{u}_i\bold{u}_i^T$$</span></p>
<p>By sorting them from largest to lowest, we can get the <span
class="math inline"><em>M</em></span> principal components by taking the
first <span class="math inline"><em>M</em></span> directions.</p>
<h4 id="minimum-error-formulation">Minimum Error Formulation</h4>
<p>PCA can be defined as the linear projection that minimizes the
average projection cost, defined as the mean squared distance between
data points and their projection</p>
<h2 id="reinforcment-learning">Reinforcment Learning</h2>
<h3 id="problem-definition">Problem definition</h3>
<p>A reinforcement learning problem is a special type of machine
learning problem. We have an agent and and environment, in a dynamic
system. The agent interacts with the environment by choosing and action,
to which the environment responds changing the state. Our goal is to
determine an optimal policy <span
class="math inline"><em>œÄ</em>‚ÄÑ:‚ÄÑ<em>X</em>‚ÄÑ‚Üí‚ÄÑ<em>A</em></span> given a
dataset $D = {&lt;x_1,a_1,r_1, x_n,a_n,r_n&gt;^{i} } $ composed by a
sequence of states, action and rewards.</p>
<h3 id="markov-property">Markov property</h3>
<p>An assumption made in RL is the markov assumption, in other words we
assume that the dynamic system does not depend on the history of states,
observations, and actions as long as the current state is known.</p>
<h3 id="mdp">MDP</h3>
<p>An MDP is process for decision making. We assume that the states are
fully observable, and we can act directly on them. Since the states are
observable all the information that we need are contained into them. An
MDP can be described ad a tuple:</p>
<p><span class="math display">$$\text{MDP} =
&lt;\bold{X},\bold{A},\delta,r&gt;$$</span></p>
<p><span class="math inline">$\bold{X}$</span> is the set of possibles
states.<span class="math inline">$\bold{A}$</span> is the set of
possibles actions. <span class="math inline"><em>Œ¥</em></span> is a
transition model and describes how the system evolve after executing
each action. <span class="math inline"><em>r</em></span> is a reward
function, after each action the system recieves a numeric feedback
signaling either a good or bad choice of action.</p>
<h3 id="definition-of-k-bandit-problem">Definition of K-Bandit
problem</h3>
<!-- https://ai.stackexchange.com/questions/27694/what-are-the-major-differences-between-multi-armed-bandits-and-the-other-well-kn#:~:text=Multi%2DArmed%20Bandit%20is%20used%20as%20an%20introductory%20problem%20to%20reinforcement%20learning%2C%20because%20it%20illustrates%20some%20basic%20concepts%20in%20the%20field%3A%20exploration%2Dexploitation%20tradeoff%2C%20policy%2C -->
<p>The K-bandit problem is a subclass of reinforcement learning
problems. The problem goes as follows:</p>
<p><strong>‚ÄúYou are faced to choose between <span
class="math inline"><em>k</em></span> options, after you choose an
option you recieve a reward, the goal is the maximize the expected
reward over a period of time‚Äù</strong></p>
<p>The K-bandit problem is also called one-state MDP because you have
only one state. The importance of the k-armed bandit is due to the fact
that illustrates some pillar concepts of reinforcement learning, such as
the exploration-exploitation trade-off, the concept of optimal policy
and more.</p>
<p>In a K-Bandit problem, we have no information about the rewards. Thus
in order to get the optimal policy we need to learn (gather informations
about <span class="math inline"><em>r</em></span> through simulation),
determine an optimal policy and only then we can make decisions.</p>
<p>Depending on how the reward function behaves, and the type of
informations about it we can define 4 solutions:</p>
<ul>
<li>If the rewards are deterministic, and known, the optimal solution is
just picking always the action that yields the highest reward. No
iterations are needed, we can skip the learning and go directly to
making decisions.</li>
<li>If the rewards are deterministic, but unknown, we need to test each
action at least one time, in order to discover the value, thus we need
at least <span class="math inline">|<em>A</em>|</span> iterations.</li>
<li>If the rewards are non-deterministic, but known, the optimal
solution can be found by picking the action which has the highest
mean.</li>
<li>If the rewards are non-deterministic and unknown, we have to iterate
through
<ul>
<li>Initialize a data structure <span
class="math inline"><em>Œò</em></span> that will gather informations
about the reward.
<ul>
<li>For each time <span class="math inline"><em>t</em></span>, until
termination:
<ul>
<li>Choose an action</li>
<li>Execute the choosen action</li>
<li>Collect the reward</li>
<li>Update <span class="math inline"><em>Œò</em></span></li>
</ul></li>
<li>Choose an optimal policy <span
class="math inline"><em>œÄ</em><sup>*</sup></span> according to <span
class="math inline"><em>Œò</em></span></li>
</ul></li>
</ul></li>
</ul>
<h3 id="q-learning">Q-Learning</h3>
<p>Is an algorithm to determine the optimal policy. We define the
Q-value as <span
class="math inline"><em>Q</em>(<em>x</em>,<em>a</em>)</span> as the
expected cumulative reward the agent will recieve by taking the action
<span class="math inline"><em>a</em></span> in state <span
class="math inline"><em>x</em></span> and following the policy there
after. For each state action pair, the Q-learning algorithm mantains a
Q-value and updates iteratively. The pseudo code is the following:</p>
<ul>
<li>For each state action pair initialize a table entry in the Q
table.</li>
<li>For each time:</li>
<li><ul>
<li>Observe the state <span class="math inline"><em>x</em></span>.</li>
<li>Choose and action <span class="math inline"><em>a</em></span>.</li>
<li>Execute the action choosen.</li>
<li>Observe the new state.</li>
<li>Collect the reward.</li>
<li>Update the table relative to <span
class="math inline"><em>x</em></span> , <span
class="math inline"><em>a</em></span> with the new reward.</li>
<li>Move to the new state.</li>
</ul></li>
<li>Choose the oprimal policy according to the Q table.</li>
</ul>
<p>If the reward functions are non deterministic or unknown the Q-value
is just an estimate. Our objective is to maximize the reward. At any
fiven time there must be an action that has the highest reward, this
action is called greedy. We can alwayas choose the action that yields
the best result (exploitation). However we might its also possible that
by choosing some suboptimal action, the overall policy is better
(exploration). At any given time you cannot choose both exploration and
exploitation thus you have a trade off. An approach that can be embedded
into the Q-learning algorithm, is that when choosing the action, we
choose with a probability of <span
class="math inline">1‚ÄÖ‚àí‚ÄÖ<em>œµ</em></span> greedy action and with a
probability of <span class="math inline"><em>œµ</em></span> a random
action choosen uniformly.</p>
<h3 id="hmm">HMM</h3>
<p>The Hidden Markov Models are a formulation of a Markov Process in
which we assume that we cannot control the evolution of the system . We
assume that a state generates an event with a certain probability, we
can observe the event but we cannot observe the state.</p>
<p><span class="math display">$$\text{HMM} = &lt;\bold{X},\bold{Z},
\pi_0&gt;$$</span></p>
<p><span class="math inline">$\bold{X}$</span> is the set of states,
which are not observable but we know that exists. <span
class="math inline">$\bold{Z}$</span> is the set of observations of the
events generated by the states. The transition model is described by
<span class="math inline">$P(\bold{x_t}|\bold{x_{t-1}})$</span>, meaning
that the current state depends on the previous state. The observation
modle instead is <span
class="math inline">$P(\bold{z_t}|\bold{x_{t-1}})$</span>, meaning that
the current observation depends from the previous state. Last <span
class="math inline"><em>œÄ</em><sub>0</sub></span> describes the
probability of starting at exactly <span class="math inline">$x_0, x_0
\in \bold{X}$</span>. Our goal is to reconstruct the states from the
observations.</p>
<h3 id="pomdp">POMDP</h3>
<p>Partially Observable Markov Decision Process are a union of the
elements of both MDP and HMM. As in the HMM we don‚Äôt assume a full
observability of the states, however as in the MDP we can act on the
environment.</p>
<p><span class="math display">$$\text{POMDP}=
&lt;\bold{X},\bold{A},\bold{Z}, \delta, r, o&gt;$$</span></p>
<p><span class="math inline">$\bold{X}$</span> is the set of states,
which are not observable but we know that exists. <span
class="math inline">$\bold{Z}$</span> is the set of observations of the
events generated by the states. We have a probability distribution
describing the probability of starting at state <span
class="math inline">$x_0, x_0 \in \bold{X}$</span>. We can act on the
environment by choosing an action in <span
class="math inline"><em>A</em></span>. The transition function <span
class="math inline"><em>Œ¥</em></span> is a probability distribution
<span
class="math inline"><em>P</em>(<em>x</em>‚Ä≤|<em>x</em>,<em>a</em>)</span>
that describes the probability of ending up on <span
class="math inline"><em>x</em>‚Ä≤</span> after executing <span
class="math inline"><em>a</em></span> on <span
class="math inline"><em>x</em></span>. <span
class="math inline"><em>o</em></span> is a probability distribution over
distributions, <span
class="math inline"><em>P</em>(<em>z</em>‚Ä≤|<em>x</em>‚Ä≤,<em>a</em>)</span>
describes the probability of observing <span
class="math inline"><em>z</em>‚Ä≤</span>, after we end up on <span
class="math inline"><em>x</em>‚Ä≤</span> by the execution of <span
class="math inline"><em>a</em></span></p>
<h2 id="neural-networks">Neural Networks</h2>
<p>The neural network models can be described at a mathematical level as
a composition of functions, where each function is defined either in the
hiden layers or the output layer. Supposing that there are <span
class="math inline"><em>H</em></span> hidden layers <span
class="math inline"><em>l</em></span>:</p>
<p><span class="math display">$$y(\bold{x};\theta) =
f^{(out)}(l^H;(\theta))\\$$</span> <span
class="math display">$$l^H(\bold{x};\theta) =
f^{(H)}(l^{(H-1)};(\theta))$$</span> <span class="math display">‚ãÆ</span>
<span class="math display">$$l^{(1)}(\bold{x};\theta) =
f^{(1)}(\bold{x};(\theta))$$</span></p>
<h3 id="backpropagation">Backpropagation</h3>
<p>The back-propagation algorithm is used to propagate gradient
computation from the cost through the whole network. It leverages the
chain rule and the fact that the Network is just a composition of
functions to determine the gradient in an efficient way. The
Backpropagation algorithm can be thought as two different steps:</p>
<h4 id="forward-step">Forward step</h4>
<p>It makes the input <span class="math inline">$\bold{x}$</span> ‚Äúflow‚Äù
through the network.</p>
<ul>
<li>For each layer
<ul>
<li>Determine the linear combination of the weights with the input and
add the bias.</li>
<li>Apply the <span
class="math inline"><em>f</em><sup>(<em>k</em>)</sup></span> function to
the linear combination</li>
<li>Use the result as input for the next layer</li>
</ul></li>
<li>Use the input to determine the error w.r.t to the true value</li>
</ul>
<h4 id="backward-step">Backward step</h4>
<p>It takes the gradient of the loss and makes it ‚Äúflow‚Äù up until the
input</p>
<ul>
<li>Determine the gradient of the loss</li>
<li>For each layer starting from the last layer:
<ul>
<li>Determine the gradient of the linear combination of inputs pre
activation</li>
<li>Determine the gradient of the linear combinations of inputs post
activation</li>
<li>Determine the gradient of the bias</li>
</ul></li>
</ul>
<h3 id="sgd">SGD</h3>
<p>Stochastic Gradient descent is an iterative method for optimization.
It updates the weights using this update rule:</p>
<p><span class="math display">$$\boldsymbol{\theta}^{(t+1)} =
\boldsymbol{\theta}^{(t)} - \eta \nabla E(\bold{\theta})$$</span></p>
<p>Where <span class="math inline"><em>Œ∑</em></span> is an
hyperparameter called learning rate.</p>
<ul>
<li>While the stopping criterion is not met:
<ul>
<li>Sample an <span class="math inline"><em>m</em></span> dimensional
subset at random from the dataset</li>
<li>Compute the gradient estimate using Backpagation of the error
averaged over the number of the samples</li>
<li>Apply the update rule</li>
</ul></li>
</ul>
<h3 id="architectural-designs">Architectural designs</h3>
<h4 id="saturation">Saturation</h4>
<p>A saturating activation functions squeeze the input. In the neural
network context, the phenomenon of saturation refers to the state in
which a neuron predominantly outputs values close to the asymptotic ends
of the bounded activation function.</p>
<h4 id="cost-function">Cost function</h4>
<p>Model implicitly defines a conditional distribution <span
class="math inline">$P(\bold{t} \mid \bold{x},
\boldsymbol{\theta})$</span>. We can define the loss function using the
Maximum likelihood principle. In particular we can just take a the
expected value of the, the minimum of negative log likelihood.</p>
<p><span class="math display">$$J(\boldsymbol{\theta}) =
\mathbb{E}_{\bold{x},\bold{t}\sim D} [- \ln P(\bold{t} \mid \bold{x},
\boldsymbol{\theta})]$$</span></p>
<p>Where the specific cost function depends on the specific model.</p>
<h5 id="regression">Regression</h5>
<p>Linear units: Identity activation function</p>
<p><span class="math display">$$y = \bold{w}^T \bold{h} + \bold{b}
$$</span></p>
<p>Assuming a Gaussian distribution noise the functional form of the
error function is the MSE.</p>
<p><span class="math display">$$J(\boldsymbol{\theta}) =
\mathbb{E}_{x,t\sim D} [\frac{1}{2} (t-
y(\bold{x_n};\boldsymbol{\theta}))^2] $$</span></p>
<p>There is no ‚Äúsqueeze‚Äù of the input, thus the units cannot
saturate.</p>
<h5 id="binary-classification">Binary classification</h5>
<p>Output units: Sigmoid activation function</p>
<p><span class="math display">$$y = \sigma(\bold{w}^T \bold{h} +
\bold{b}) $$</span></p>
<p>The functional form of the error function is the Binary
cross-entropy.</p>
<p><span
class="math display"><em>J</em>(<strong>Œ∏</strong>)‚ÄÑ=‚ÄÑùîº<sub><em>x</em>,‚ÄÜ<em>t</em>‚ÄÑ‚àº‚ÄÑ<em>D</em></sub>[‚àíln<em>p</em>(<em>t</em>‚à£<em>x</em>)]</span></p>
<p>Output unit saturates only when it gives the correct answer.</p>
<h5 id="multi-class-classification">Multi-class classification</h5>
<p>Output units: Softmax activation functions</p>
<p><span class="math display">$$y_i = \text{softmax}(\alpha^{(i)}) =
\frac{\exp(\alpha^{(i)})}{\sum_j \exp(\alpha_j)} $$</span></p>
<p>Loss function: Categorical cross-entropy</p>
<p><span
class="math display"><em>J</em><sub><em>i</em></sub>(<strong>Œ∏</strong>)‚ÄÑ=‚ÄÑùîº<sub><em>x</em>,‚ÄÜ<em>t</em>‚ÄÑ‚àº‚ÄÑ<em>D</em></sub>[‚àílnsoftmax(<em>Œ±</em><sup>(<em>i</em>)</sup>)]</span></p>
<p>with <span class="math inline">$( \alpha^{(i)} = \bold{w}_i^T
\bold{h} + \bold{b}_i)$</span>. Output units saturate only when there
are minimal errors.</p>
<h3 id="cnn">CNN</h3>
<p>A Convolutional Neural Network are Neural Networks that uses
convolution instead of matrix multiplication in at least one layer. They
are specialized for processing data that present a grid-like
topology.</p>
<h4 id="regularization-in-a-cnn">Regularization in a CNN</h4>
<p>Convolutional Neural Networks legerages two main ideas to overcome
overfitting.</p>
<ul>
<li>Sparse interactions: Traditional NN uses matrix multiplication
between a layer and the next, meaning that each parameter describes an
interaction between input and output. Convolutional Neural Networks
howevers use sparse interactions. This is accomplished by makingthe
kernel smaller than the input.</li>
<li>Parameter sharing: We force a set of parameters to have the same
value</li>
</ul>
<h4
id="determination-of-number-of-trainable-parameters-for-a-cnn">Determination
of number of trainable parameters for a CNN</h4>
<p><span class="math display">$$w_{out} = \frac{w_{in} - w_{k} +
2p}{s}+1 $$</span> <span class="math display">$$h_{out} = \frac{h{in} -
h_{k} + 2p}{s}+1 $$</span> <span
class="math display">‚ÄÖ‚à£‚ÄÖ<em>Œ∏</em>‚ÄÖ‚à£‚ÄÖ‚ÄÑ=‚ÄÑ<em>w</em><sub><em>k</em></sub>‚ÄÖ√ó‚ÄÖ<em>h</em><sub><em>k</em></sub>‚ÄÖ√ó‚ÄÖ<em>d</em><sub><em>i</em><em>n</em></sub>‚ÄÖ√ó‚ÄÖ<em>d</em><sub><em>o</em><em>u</em><em>t</em></sub>‚ÄÖ+‚ÄÖ<em>d</em><sub><em>o</em><em>u</em><em>t</em></sub></span></p>
<p>Necessary padding:</p>
<p><span class="math display">$$\left\lfloor \frac{wk}{2} \right\rfloor
$$</span></p>
<h3 id="autoencoder">Autoencoder</h3>
<p>An autoencoder is a special type of neural network that is trained to
copy its input to its output.It has the following properties</p>
<ol type="1">
<li>combination of two NN, an encoder and a decoder.</li>
<li>trained based on reconstruction loss</li>
<li>provides low dimensional representation</li>
<li>Bottleneck concept, which learn to reconstruct input minimizing a
loss function</li>
<li>Autoencoders can be seen as a method for non-linear principal
component analysis</li>
</ol>
<h2 id="general">General</h2>
<h3 id="difference-between-pca-and-autoencoder">Difference between PCA
and Autoencoder</h3>
<p>They both perform dimensionality reduction, however they do it in two
different ways. The difference between PCA and autoencoders is that PCA
perform a linear trasformation, while autoencoders perform a non linear
transformation</p>
<h3 id="ensambles">Ensambles</h3>
<p>The general idea of examples is to combine different models into a
single best model</p>
<h4 id="boosting">Boosting</h4>
<p>Is a sequential approach to ensembles. Each learner is trained on the
dataset weighted on previous mistakes. Our goal is to minimize the
misclassified instances, thus after each iteration we give higher weight
to misclassified instances, thus specializing each learner in the set of
instances that was previously misclassified.</p>
<h5 id="adaboost">AdaBoost</h5>
<p>Assuming you have <span class="math inline"><em>M</em></span> weak
learners, learnes that perform at least a little bit better than a
random guesser Given <span
class="math inline">(<em>D</em>={(<em>x</em><sub>1</sub>,<em>t</em><sub>1</sub>),‚Ä¶,(<em>x</em><sub><em>N</em></sub>,<em>t</em><sub><em>N</em></sub>)}),‚ÄÜ<em>w</em><em>h</em><em>e</em><em>r</em><em>e</em>(<em>x</em><sub><em>n</em></sub>‚àà<em>X</em>,<em>t</em><sub><em>n</em></sub>‚àà‚àí1,‚ÄÜ‚ÄÖ+‚ÄÖ1)</span></p>
<ul>
<li><p>Uniformly initialize the weights.</p></li>
<li><p>For <span
class="math inline">(<em>m</em>=1,‚Ä¶,<em>M</em>)</span>:</p>
<ul>
<li>Train a weak learner <span
class="math inline">(<em>y</em><sub><em>m</em></sub>(<em>x</em>))</span>
by minimizing the weighted error function.</li>
<li>Evaluate alpha, the confidence that we put on the predictive
power</li>
<li>Update the data weighting coefficients</li>
</ul></li>
<li><p>Output the final classifier</p></li>
</ul>
